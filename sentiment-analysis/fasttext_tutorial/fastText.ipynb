{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial : fastText\n",
    "\n",
    "### What is fastText ?\n",
    "**fastText** stands for two different -but related- algorithms from Facebook Research. Both algorithms were developped for text classification, but the name \"fastText\" may either refer to the classification technique or the embedding technique that the former is based on. Let's quicly talk about the embedding technique.\n",
    "\n",
    "Unlike other classic word embedding techniques such as Word2vec or Glove, fastText can work at the **character-level**. This means that instead of learning embeddings for words or word N-grams (N successive words), fastText is able to learn embeddings for **character N-grams**. For example:\n",
    "\n",
    "- Instead of learning a word representation for \"thanks\", the algorithm may learn one for \"< thank\", \"thanks\" and \"hanks >\"\n",
    "\n",
    "\n",
    "- Note that the word representation for the character N-gram \"hanks >\" will be different from the one of the word \"< hanks >\" from Tom Hanks\n",
    "\n",
    "\n",
    "- Let's assume the model has never seen the word \"< thankful >\". It is then able to produce a good embedding using the word vector for \"< thank\".\n",
    "\n",
    "Using these word embeddings along with many optimization tricks, the fastText classification model trains a simple linear model that is extremely fast both at training and prediction time.\n",
    "\n",
    "For more information abour the theory, please refer to the following papers:\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1607.04606) P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information  \n",
    "[[2]](https://arxiv.org/abs/1607.01759) A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use FastText for text classification ?\n",
    "\n",
    "fastText was developped to be used in a command line interface, but thankfully there is an official python wrapper that will allow us to train/load/compress/save models. This tutorial is in Python 2.7, but everything should work on python 3. The only thing to be cautious about is that all the strings are \"utf-8\" encoded.\n",
    "\n",
    "Requirements: To use fastText we will need to install the following packages `numpy` `scipy` `pybind11` `fasttext`\n",
    "\n",
    "- First we install the dependencies by running in a terminal `pip install numpy scipy pybind11`\n",
    "\n",
    "\n",
    "- Then we install the actual FastText package from it's repository: `pip install git+https://github.com/facebookresearch/fastText.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to import FastText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works ! Let's move on to actually using this library.\n",
    "\n",
    "## fastText : using a pre-trained model\n",
    "\n",
    "fastText provides us with many models that were pre-trained on many different tasks. These tasks range from **sentiment analysis** through **topic classification** to **language identification**. To access the list of available models please refer to [this page](https://github.com/facebookresearch/fastText/blob/master/docs/supervised-models.md). Every pre-trained model is avalable in two versions, a more accurate but larger version (.bin) and a compressed, lighter but slightly less accurate version (.ftz).\n",
    "\n",
    "Let's load the compressed **sentiment analysis** classifier that were trained on **amazon reviews**. First we need to download the model by running:  \n",
    "`wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/supervised_models/amazon_review_full.ftz`\n",
    "\n",
    "Once downloaded, we can load the model using the `load_model` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "MODEL_DIR_PATH = \"yourpath/pretrained_models/fasttext/\"\n",
    "\n",
    "model = fastText.load_model(os.path.join(MODEL_DIR_PATH, \"amazon_review_full.ftz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it ! We now have a model that is able to perform sentiment analysis on any text english text you have.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'__label__3',), array([0.84078544]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"This tutorial is amazing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs two things, a label and a number. The label is the predicted category and it depends on the task the model was trained on. Here we used a model trained on the amazon reviews dataset which learned to predict a sentiment value ranging from 1 to 5. The other output is a probability. The higher it is, the more the model is confident in its prediction.\n",
    "\n",
    "In this example we see that the model predicts a sentiment of 3 (meaning a rather neutral sentence) with a confidence of 84%. This is weird because the sentence is clearly positive. So what happened ?\n",
    "\n",
    "The problem is that we didn't pre-process the sentence properly. In fact, all these pretrained models were trained on texts that were preprocessed so that all the strings are lowered and all the punctuation is removed. Let's write a quick pre-processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "maketrans = string.maketrans\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing to clean text data.\n",
    "    \n",
    "    In particular:\n",
    "    - lowers the string\n",
    "    - removes the character [']\n",
    "    - replaces punctuation characters with spaces\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\'\", \"\", text)  # remove the character [']\n",
    "\n",
    "    # removing the punctuation\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    split = \" \"\n",
    "\n",
    "    if isinstance(text, unicode):\n",
    "        translate_map = dict((ord(c), unicode(split)) for c in filters)\n",
    "        text = text.translate(translate_map)\n",
    "    elif len(split) == 1:\n",
    "        translate_map = maketrans(filters, split * len(filters))\n",
    "        text = text.translate(translate_map)\n",
    "    else:\n",
    "        for c in filters:\n",
    "            text = text.replace(c, split)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a prediction function that pre-processes the input text before feeding it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment = lambda s: model.predict(clean_text(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's retry the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'__label__4',), array([0.52384621]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This tutorial is amazing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better! The model predicts a more positive sentiment, but it still does so with a low confidence... This may be because of the word \"tutorial\" that might not be in the model's vocabulary. Let's try again without the word \"tutorial\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'__label__5',), array([0.84315604]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This is amazing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were right! We now predict a highly positive sentiment with a pretty good confidence. Let's try some other sentences before we move on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'__label__3',), array([0.81006908]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This tutorial is okay.\")  # neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'__label__1',), array([0.95363003]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This tutorial is horrible.\")  # highly negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText : train a fresh model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can train our own fastText classifier on out own data. The process is very simple. The only thing we need to do is to create a file where each sentence is on a different line, and the lines start with \"\\__label__\\\" then the category. For example:\n",
    "\n",
    "- Assume we have a dataset with samples (review, sentiment)\n",
    "- A sample of this dataset is (\"This pizza is amazing!\", \"1\")\n",
    "- The corresponding line in the file should be: \\__label\\__1 this pizza is amazing\n",
    "\n",
    "Note that we preprocessed the text before making our training data file, this is not necessary but highly recommended and will boost the performance of the model.\n",
    "\n",
    "Let's train a fastText classifier on the [IMDB movie reviews dataset](http://ai.stanford.edu/~amaas/data/sentiment/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATA_DIR_PATH = \"path_to_IMDB_csv_files\"\n",
    "\n",
    "imdb_train = pd.read_csv(os.path.join(DATA_DIR_PATH, \"imdb_reviews.train.csv\"))\n",
    "imdb_test = pd.read_csv(os.path.join(DATA_DIR_PATH, \"imdb_reviews.test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A preview of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This show is wonderful. It has some of the bes...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact, Marc Blitzstein's off-Broadway adapta...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This so called remake is terrible. I went to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a nice little movie with a nice story,...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I managed to sneak away one night and go to th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment  polarity\n",
       "0  This show is wonderful. It has some of the bes...         10         1\n",
       "1  In fact, Marc Blitzstein's off-Broadway adapta...          4         0\n",
       "2  This so called remake is terrible. I went to s...          1         0\n",
       "3  This is a nice little movie with a nice story,...          8         1\n",
       "4  I managed to sneak away one night and go to th...          1         0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has two sentiment columns, one ranging from 1 to 10 and another binarized into 0-1. Let's use the `polarity` column. We need to change this column by prepending the word \"\\__label\\__\" to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train.polarity = imdb_train.polarity.apply(lambda v: \"__label__\" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we also need to pre-process the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train.sentence = imdb_train.sentence.apply(lambda s: clean_text(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at these columns before writing them into our training file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>this show is wonderful  it has some of the bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>in fact  marc blitzsteins off broadway adaptat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>this so called remake is terrible  i went to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>this is a nice little movie with a nice story ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>i managed to sneak away one night and go to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     polarity                                           sentence\n",
       "0  __label__1  this show is wonderful  it has some of the bes...\n",
       "1  __label__0  in fact  marc blitzsteins off broadway adaptat...\n",
       "2  __label__0  this so called remake is terrible  i went to s...\n",
       "3  __label__1  this is a nice little movie with a nice story ...\n",
       "4  __label__0  i managed to sneak away one night and go to th..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train[[\"polarity\", \"sentence\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to use UTF-8 encoding !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train[[\"polarity\", \"sentence\"]].to_csv(\n",
    "    \"yourpath/imdb.train\",\n",
    "    sep=\" \", encoding=\"utf-8\",\n",
    "    index=False, header=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training file, we can use the `train_supervised` to train a fastText classifier. This function can take many training parameters and we will just use the parameters in the fastText online doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training took 31.76 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "model = fastText.train_supervised(\n",
    "    input=\"yourpath/imdb.train\", epoch=25,\n",
    "    lr=1.0, wordNgrams=2, verbose=2, minCount=1)\n",
    "\n",
    "print(\"The training took {:.2f} seconds\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, we trained a whole classifier on 25,000 english reviews in 30 seconds. This is way faster than any other deep learning based classification techniques, but does it work as well ?\n",
    "\n",
    "Let's pre-process our test sentences and predict their sentiment polarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = imdb_test.sentence.apply(lambda s: clean_text(s).decode(\"utf-8\")).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction took 3.79 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predicted_labels, probabilities = model.predict(list(test_sentences))\n",
    "\n",
    "print(\"Prediction took {:.2f} seconds\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to quickly convert the predicted labels (strings that begin with \\__label\\__) to numerical polarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "predicted_labels = np.vectorize(lambda s: int(s[-1]))(np.array(predicted_labels).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the accuracy on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the freshly trained model is: 89.64\n"
     ]
    }
   ],
   "source": [
    "actual_labels = imdb_test.polarity.values\n",
    "\n",
    "print(\"The accuracy of the freshly trained model is: {:.2f}\".format((predicted_labels==actual_labels).mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89.64 % ! This is a great accuracy on this dataset. Keep in mind that we didn't fine-tune anything and that the training only took 30 seconds.\n",
    "\n",
    "These are great results, but at which cost ? Let's quickly check the size of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"yourpath/imdb.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 835 mb.\n"
     ]
    }
   ],
   "source": [
    "print(\"The model size is {:.0f} mb.\".format(os.stat(\"yourpath/imdb.bin\").st_size/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "800+ megabytes is a big model. Fortunately, fastText provides a method for compressing a model without loosing too much performance:  \n",
    "(please refer to [this paper](https://arxiv.org/abs/1612.03651) for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing the model took 80.19 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "model.quantize(input=\"yourpath/imdb.train\", qnorm=True, retrain=True, cutoff=100000)\n",
    "\n",
    "print(\"Compressing the model took {:.2f} seconds\".format(time.time() - t0))\n",
    "\n",
    "model.save_model(\"/data/helboukkouri/misc/imdb.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 6 mb.\n"
     ]
    }
   ],
   "source": [
    "print(\"The model size is {:.0f} mb.\".format(os.stat(\"yourpath/imdb.ftz\").st_size/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compression took a little bit more than a minute but the result is worth it, the model is now 6 mb !\n",
    "\n",
    "But did we loose any accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the freshly trained model is: 89.45\n"
     ]
    }
   ],
   "source": [
    "predicted_labels, probabilities = model.predict(list(test_sentences))\n",
    "predicted_labels = np.vectorize(lambda s: int(s[-1]))(np.array(predicted_labels).ravel())\n",
    "actual_labels = imdb_test.polarity.values\n",
    "\n",
    "print(\"The accuracy of the freshly trained model is: {:.2f}\".format((predicted_labels==actual_labels).mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is still over 89% so we won over 800mb of space for the very small price (on a balanced dataset) of ~ 0.2% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "creator": "hicham",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
